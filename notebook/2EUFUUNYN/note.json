{
  "paragraphs": [
    {
      "text": "%sh\n# wget http://host.docker.internal:8900/exercises/data/all-shakespeare.txt\npwd",
      "user": "anonymous",
      "dateUpdated": "2019-12-02 22:15:45.188",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "/zeppelin\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575324915207_-488875171",
      "id": "20191202-221515_153989307",
      "dateCreated": "2019-12-02 22:15:15.207",
      "dateStarted": "2019-12-02 22:15:45.207",
      "dateFinished": "2019-12-02 22:15:45.225",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import scala.io.Source \nval inputFile \u003d \"/zeppelin/all-shakespeare.txt\"\nval text \u003d Source.fromFile(inputFile)",
      "user": "anonymous",
      "dateUpdated": "2019-12-02 22:15:56.040",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import scala.io.Source\ninputFile: String \u003d /zeppelin/all-shakespeare.txt\ntext: scala.io.BufferedSource \u003d non-empty iterator\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575324663632_-1114582375",
      "id": "20191202-221103_1686427376",
      "dateCreated": "2019-12-02 22:11:03.632",
      "dateStarted": "2019-12-02 22:15:56.057",
      "dateFinished": "2019-12-02 22:15:56.187",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "\nval input \u003d sc.textFile(inputFile)\n\ndef lower(string: String) \u003d string.toLowerCase\n\n// The following return the same results:\ninput.map(lower)\ninput.map(line \u003d\u003e lower(line))\n\nval lines \u003d input.map(line \u003d\u003e line.toLowerCase)\n\nval wordCount \u003d lines.flatMap(line \u003d\u003e line.split(\"\"\"\\W+\"\"\"))\n                     .groupBy(word \u003d\u003e word)\n                     .mapValues(seq \u003d\u003e seq.size)\n                     .cache()",
      "user": "anonymous",
      "dateUpdated": "2019-12-02 22:16:14.997",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "inputFile: String \u003d hdfs://host.docker.internal:8020/training/all-shakespeare.txt\ninput: org.apache.spark.rdd.RDD[String] \u003d hdfs://host.docker.internal:8020/training/all-shakespeare.txt MapPartitionsRDD[103] at textFile at \u003cconsole\u003e:27\nlower: (string: String)String\nlines: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[106] at map at \u003cconsole\u003e:35\nwordCount: org.apache.spark.rdd.RDD[(String, Int)] \u003d MapPartitionsRDD[110] at mapValues at \u003cconsole\u003e:39\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575322530171_2023738634",
      "id": "20191202-213530_1465362785",
      "dateCreated": "2019-12-02 21:35:30.171",
      "dateStarted": "2019-12-02 22:04:19.952",
      "dateFinished": "2019-12-02 22:04:20.472",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "wordCount.foreach(println)\n",
      "user": "anonymous",
      "dateUpdated": "2019-12-02 22:04:40.891",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, 172.18.0.3, executor 0): org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-246377680-127.0.0.1-1556765200086:blk_1073742710_1888 file\u003d/training/all-shakespeare.txt\n\tat org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:984)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:642)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:882)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n\tat org.apache.hadoop.mapred.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:208)\n\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:246)\n\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:293)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:224)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:927)\n  at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:925)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.foreach(RDD.scala:925)\n  ... 47 elided\nCaused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-246377680-127.0.0.1-1556765200086:blk_1073742710_1888 file\u003d/training/all-shakespeare.txt\n  at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:984)\n  at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:642)\n  at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:882)\n  at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)\n  at java.io.DataInputStream.read(DataInputStream.java:149)\n  at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n  at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)\n  at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)\n  at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n  at org.apache.hadoop.mapred.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:208)\n  at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:246)\n  at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)\n  at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:293)\n  at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:224)\n  at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n  at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n  at org.apache.spark.scheduler.Task.run(Task.scala:121)\n  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://b13f899846ef:4040/jobs/job?id\u003d0"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1575322650512_-709147398",
      "id": "20191202-213730_165894196",
      "dateCreated": "2019-12-02 21:37:30.512",
      "dateStarted": "2019-12-02 22:04:40.921",
      "dateFinished": "2019-12-02 22:06:53.944",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "user": "anonymous",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1575324272072_-790426412",
      "id": "20191202-220432_2017688197",
      "dateCreated": "2019-12-02 22:04:32.072",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Exercises/ex1",
  "id": "2EUFUUNYN",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "sh:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}